{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "authorship_tag": "ABX9TyPswMm0KKtnmgJ55JVpkYIZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Cassandramack97/APS360/blob/main/APS360BaseModelWFeatureScaling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Code fomr MIE370 project 1 for reference"
      ],
      "metadata": {
        "id": "PXi8R2Os0EZL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CX4uAS3P8d-N"
      },
      "outputs": [],
      "source": [
        "### YOUR CODE HERE ###\n",
        "#from sklearn.ensemble import RandomForestClassifier\n",
        "#from sklearn.model_selection import RandomizedSearchCV\n",
        "#from scipy.stats import randint\n",
        "\n",
        "#RFClassifier = RandomForestClassifier()\n",
        "\n",
        "#paramsdistRF = {'max_features': ['sqrt', 'log2', 0.1, 0.25, 0.5], 'n_estimators': randint(10, 200)}\n",
        "\n",
        "#randSearchRF = RandomizedSearchCV(RFClassifier, param_distributions=paramsdistRF, cv=5, verbose=2, random_state=42, n_jobs=-1, n_iter=500)\n",
        "\n",
        "#randSearchRF.fit(stand_training_x, y_train)\n",
        "\n",
        "#bestScoreRF = randSearchRF.best_score_\n",
        "#bestParamsRF = randSearchRF.best_params_\n",
        "\n",
        "#printing satement like the example\n",
        "#print(f\"Best n_estimators: {bestParamsRF['n_estimators']}, Best Max Features: {bestParamsRF['max_features']}, Best Accuracy: {bestScoreRF}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "getting access to the data"
      ],
      "metadata": {
        "id": "vX8qltY93Jzt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#mount drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "953qSQYh1iSU",
        "outputId": "8d09098a-637d-4732-98f0-027b5ad2c898"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Making sure that it can access it\n",
        "#for this to work add a shortcut to the folder in your drive\n",
        "!ls \"/content/drive/MyDrive/APS360_Project\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aZqdt6zH2C99",
        "outputId": "73892f72-ad19-4417-a65a-5cb650415b38"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\"FMA Bullshit Data Cleaning'\"\t      tensors2_val     tensors_val3\t     train_metadata3.csv\n",
            "'Good Data Cleaning and Processing'   tensors_test     test2_metadata.csv    train_metadata.csv\n",
            " Michael_Full_Cleaned_Dataset.csv     tensors_test3    test_metadata3.csv    val2_metadata.csv\n",
            " songs_with_details.csv\t\t      tensors_train    test_metadata.csv     val_metadata3.csv\n",
            " tensors2_test\t\t\t      tensors_train3   tracks_features.csv   val_metadata.csv\n",
            " tensors2_train\t\t\t      tensors_val      train2_metadata.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from scipy.stats import randint\n",
        "from sklearn.metrics import make_scorer, mean_squared_error\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n"
      ],
      "metadata": {
        "id": "p4tF2d2N0JSZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Actually extracting features from the waveform files\n",
        "\n",
        "####Saving this code in case we need it but the files should be feature tensors now\n",
        "####sample rate prolly needs to be validated and explaine or tuned\n",
        "\n",
        "import librosa\n",
        "\n",
        "def extract_mfcc_with_timing(waveform, sr=22050, n_mfcc=13):\n",
        "  mfcc = librosa.feature.mfcc(y=waveform, sr=sr, n_mfcc=n_mfcc)\n",
        "  return mfcc  # Shape will be (n_mfcc, num_frames)\n",
        "\n",
        "def extract_chroma_with_timing(waveform, sr=22050):\n",
        "  chroma = librosa.feature.chroma_stft(y=waveform, sr=sr)\n",
        "  return chroma  # Shape will be (12, num_frames) for chroma\n",
        "\n",
        "def extract_spectral_contrast_with_timing(waveform, sr=22050):\n",
        "  spectral_contrast = librosa.feature.spectral_contrast(y=waveform, sr=sr)\n",
        "  return spectral_contrast  # Shape will be (num_bands, num_frames)\n",
        "\n",
        "def extract_tonnetz_with_timing(waveform, sr=22050):\n",
        "  tonnetz = librosa.feature.tonnetz(y=waveform, sr=sr)\n",
        "  return tonnetz  # Shape will be (4, num_frames) for tonnetz\n",
        "\n",
        "def extract_zero_crossing_rate_with_timing(waveform):\n",
        "  zero_crossing_rate = librosa.feature.zero_crossing_rate(y=waveform)\n",
        "  return zero_crossing_rate  # Shape will be (1, num_frames)\n",
        "\n",
        "def extract_features_with_timing(waveform, sr=22050):\n",
        "    mfcc = extract_mfcc_with_timing(waveform, sr)\n",
        "    chroma = extract_chroma_with_timing(waveform, sr)\n",
        "    spectral_contrast = extract_spectral_contrast_with_timing(waveform, sr)\n",
        "    tonnetz = extract_tonnetz_with_timing(waveform, sr)\n",
        "    zero_crossing = extract_zero_crossing_rate_with_timing(waveform)\n",
        "    \n",
        "    # Stack features along the feature dimension (axis=0)\n",
        "    features = np.vstack([mfcc, chroma, spectral_contrast, tonnetz, zero_crossing])\n",
        "    \n",
        "    #time steps is the first dimension now\n",
        "    #(time steps, features)\n",
        "    return features.T\n"
      ],
      "metadata": {
        "id": "VUOr8yNOP7pT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "rnn is designed to take the variable length tensors but the rf regressor isn't so we have to convert them into fixed length vectors"
      ],
      "metadata": {
        "id": "x60lkS99HgTz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "rf also cant deal w temporal information so aggregating features is the best way to retain all the info and get fixed length vectors"
      ],
      "metadata": {
        "id": "e1uWoQ6ZIxhm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install unidecode\n",
        "from unidecode import unidecode\n",
        "\n",
        "#folder path\n",
        "tensorFolderPath = '/content/drive/MyDrive/APS360_Project/tensors_train'\n",
        "\n",
        "for file in os.listdir(tensorFolderPath):\n",
        "\n",
        "    normalizedName = unidecode(file)\n",
        "\n",
        "    if file != normalizedName:\n",
        "\n",
        "        originalFile = os.path.join(tensorFolderPath, file)\n",
        "        normalizedFile = os.path.join(tensorFolderPath, normalizedName)\n",
        "\n",
        "        #rename the file\n",
        "        os.rename(originalFile, normalizedFile)\n",
        "        print(f\"Renamed: {file} -> {normalizedName}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qqf3WL3ChkSk",
        "outputId": "776b153d-78c4-4e78-b46f-47125ce4775b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting unidecode\n",
            "  Downloading Unidecode-1.3.8-py3-none-any.whl.metadata (13 kB)\n",
            "Downloading Unidecode-1.3.8-py3-none-any.whl (235 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/235.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.2/235.5 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.5/235.5 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: unidecode\n",
            "Successfully installed unidecode-1.3.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#folder path\n",
        "#tensorFolderPath = '/content/drive/MyDrive/APS360_Project/tensors_train'\n",
        "\n",
        "#feature tensors\n",
        "trainData = [file for file in os.listdir(tensorFolderPath)]\n",
        "\n",
        "#getting the labels\n",
        "labelsDF = pd.read_csv('/content/drive/MyDrive/APS360_Project/train_metadata.csv')\n",
        "\n",
        "#removing the accents from the labels\n",
        "labelsDF[labelsDF.columns[0]] = labelsDF[labelsDF.columns[0]].apply(unidecode)\n",
        "\n",
        "#making sure the labels match the files\n",
        "matchingLabelsDF = labelsDF[labelsDF[labelsDF.columns[0]].isin(trainData)]\n",
        "\n",
        "#extracting the features and converting them to a numpy array for the rf\n",
        "yTrain = matchingLabelsDF.iloc[:, 1:].values\n",
        "\n",
        "scalerY = StandardScaler()\n",
        "yTrainScaled = scalerY.fit_transform(yTrain)"
      ],
      "metadata": {
        "id": "PdQp13aMA26C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "yTrain.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K8yO0HZXercn",
        "outputId": "3ee2b32f-88e6-4c94-cf6f-db119fb53930"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(140, 11)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For debugging to see what files/labels weren't matching"
      ],
      "metadata": {
        "id": "LtkBA0g_gr5A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "missing_files = [tensorName for tensorName in trainData if tensorName not in matchingLabelsDF[matchingLabelsDF.columns[0]].values]\n",
        "print(f\"Number of missing files: {len(missing_files)}\")\n",
        "print(\"First few missing files:\", missing_files[:10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8U2KkMnvbgqA",
        "outputId": "d2890de0-9a1b-48ce-b06d-6254df03c0aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of missing files: 0\n",
            "First few missing files: []\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "missing_labels = [tensorName for tensorName in matchingLabelsDF[matchingLabelsDF.columns[0]].values if tensorName not in trainData]\n",
        "print(f\"Number of missing labels: {len(missing_labels)}\")\n",
        "print(\"First few missing labels:\", missing_labels[:10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JwjP6hT7byQ4",
        "outputId": "69d1fed8-d8b3-47b3-ebc8-8b9dadcc298c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of missing labels: 0\n",
            "First few missing labels: []\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#convert tensors\n",
        "def convertToVector(tensor):\n",
        "\n",
        "  #we have to aggregate the features over time\n",
        "  #so gotta get the std dev and mean\n",
        "  meanFeatures = np.mean(tensor, axis=0)\n",
        "  stdFeatures = np.std(tensor, axis=0)\n",
        "\n",
        "  #concatenate into a single feature vector that can be taken in by the rf\n",
        "  return np.concatenate((meanFeatures, stdFeatures))\n",
        "\n",
        "#to store the vectors\n",
        "vectorList = []\n",
        "\n",
        "#converting to the vectors for rf\n",
        "for tensorPath in trainData:\n",
        "\n",
        "  fullPath = os.path.join(tensorFolderPath, tensorPath)\n",
        "\n",
        "  tensor = np.load(fullPath)\n",
        "\n",
        "  vectorList.append(convertToVector(tensor))\n",
        "\n",
        "scalerX = StandardScaler()\n",
        "xTrain = np.array(vectorList)\n",
        "xTrainScaled = scalerX.fit_transform(xTrain)"
      ],
      "metadata": {
        "id": "IsJJ-tyoH500"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "xTrain"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nAceCedCn2in",
        "outputId": "65c9f281-d8b7-4236-ea88-879097192fc5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-4.70133055e+01,  7.61315917e+01, -2.63300001e+01, ...,\n",
              "         4.86488789e-02,  4.20257705e-02,  3.36938167e-02],\n",
              "       [-2.55471597e+02,  8.26372579e+01, -8.42534467e+00, ...,\n",
              "         6.26157404e-02,  7.16114163e-02,  5.83521786e-02],\n",
              "       [-5.25387701e+00,  8.14500267e+01, -2.93795480e+01, ...,\n",
              "         2.15557996e-02,  2.05794249e-02,  4.40538322e-02],\n",
              "       ...,\n",
              "       [-1.59306451e+02,  1.01599863e+02,  3.78970325e+00, ...,\n",
              "         2.76581058e-02,  2.98427170e-02,  4.66764535e-02],\n",
              "       [-2.77628002e+02,  1.33452131e+02, -1.68527997e+01, ...,\n",
              "         6.12851580e-02,  6.93194129e-02,  3.19360539e-02],\n",
              "       [-3.06779555e+02,  1.42870791e+02, -2.10423597e-01, ...,\n",
              "         6.28015282e-02,  6.43143522e-02,  3.06264456e-02]])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For validation and testing now"
      ],
      "metadata": {
        "id": "dSb2s-vJl7e3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#folder path\n",
        "valTensorFolderPath = '/content/drive/MyDrive/APS360_Project/tensors_val'\n",
        "\n",
        "#feature tensors\n",
        "valData = [file for file in os.listdir(valTensorFolderPath)]\n",
        "\n",
        "for file in os.listdir(valTensorFolderPath):\n",
        "\n",
        "    normalizedName = unidecode(file)\n",
        "\n",
        "    if file != normalizedName:\n",
        "\n",
        "        originalFile = os.path.join(valTensorFolderPath, file)\n",
        "        normalizedFile = os.path.join(valTensorFolderPath, normalizedName)\n",
        "\n",
        "        #rename the file\n",
        "        os.rename(originalFile, normalizedFile)\n",
        "        print(f\"Renamed: {file} -> {normalizedName}\")"
      ],
      "metadata": {
        "id": "zuNepQ1Ul6rW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#getting the labels\n",
        "valLabelsDF = pd.read_csv('/content/drive/MyDrive/APS360_Project/val_metadata.csv')\n",
        "\n",
        "#removing the accents from the labels\n",
        "valLabelsDF[valLabelsDF.columns[0]] = valLabelsDF[valLabelsDF.columns[0]].apply(unidecode)\n",
        "\n",
        "#making sure the labels match the files\n",
        "matchingValLabelsDF = valLabelsDF[valLabelsDF[valLabelsDF.columns[0]].isin(valData)]\n",
        "\n",
        "#extracting the features and converting them to a numpy array for the rf\n",
        "yVal = matchingValLabelsDF.iloc[:, 1:].values\n",
        "\n",
        "yValScaled = scalerY.fit_transform(yVal)"
      ],
      "metadata": {
        "id": "VtTtGp_wmc3A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#to store the vectors\n",
        "valVectorList = []\n",
        "\n",
        "#converting to the vectors for rf\n",
        "for tensorPath in valData:\n",
        "\n",
        "  fullValPath = os.path.join(valTensorFolderPath, tensorPath)\n",
        "\n",
        "  tensor = np.load(fullValPath)\n",
        "\n",
        "  valVectorList.append(convertToVector(tensor))\n",
        "\n",
        "xVal = np.array(valVectorList)\n",
        "xValScaled = scalerX.fit_transform(xVal)"
      ],
      "metadata": {
        "id": "PTKC6g_Snp2H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#folder path\n",
        "testTensorFolderPath = '/content/drive/MyDrive/APS360_Project/tensors_test'\n",
        "\n",
        "#feature tensors\n",
        "testData = [file for file in os.listdir(testTensorFolderPath)]\n",
        "\n",
        "for file in os.listdir(testTensorFolderPath):\n",
        "\n",
        "    normalizedName = unidecode(file)\n",
        "\n",
        "    if file != normalizedName:\n",
        "\n",
        "        originalFile = os.path.join(testTensorFolderPath, file)\n",
        "        normalizedFile = os.path.join(testTensorFolderPath, normalizedName)\n",
        "\n",
        "        #rename the file\n",
        "        os.rename(originalFile, normalizedFile)\n",
        "        print(f\"Renamed: {file} -> {normalizedName}\")"
      ],
      "metadata": {
        "id": "n-PVkXqhmDqf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#getting the labels\n",
        "testLabelsDF = pd.read_csv('/content/drive/MyDrive/APS360_Project/test_metadata.csv')\n",
        "\n",
        "#removing the accents from the labels\n",
        "testLabelsDF[testLabelsDF.columns[0]] = testLabelsDF[testLabelsDF.columns[0]].apply(unidecode)\n",
        "\n",
        "#making sure the labels match the files\n",
        "matchingTestLabelsDF = testLabelsDF[testLabelsDF[testLabelsDF.columns[0]].isin(testData)]\n",
        "\n",
        "#extracting the features and converting them to a numpy array for the rf\n",
        "yTest = matchingTestLabelsDF.iloc[:, 1:].values\n",
        "\n",
        "yTestScaled = scalerY.fit_transform(yTest)"
      ],
      "metadata": {
        "id": "2FmSyQwVmeix"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#to store the vectors\n",
        "testVectorList = []\n",
        "\n",
        "#converting to the vectors for rf\n",
        "for tensorPath in testData:\n",
        "\n",
        "  fullTestPath = os.path.join(testTensorFolderPath, tensorPath)\n",
        "\n",
        "  tensor = np.load(fullTestPath)\n",
        "\n",
        "  testVectorList.append(convertToVector(tensor))\n",
        "\n",
        "xTest = np.array(testVectorList)\n",
        "xTestScaled = scalerX.fit_transform(xTest)"
      ],
      "metadata": {
        "id": "3I_75hRiwTqL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "actual model"
      ],
      "metadata": {
        "id": "YgW8Pe9cmj7K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#defining the output labels\n",
        "outputLabels = [\"valence\", \"liveness\", \"time_signature\", \"energy\", \"speechiness\",\n",
        "                 \"instrumentalness\", \"acousticness\", \"tempo\", \"loudness\", \"danceability\", \"key\"]\n",
        "\n",
        "#define the regressor\n",
        "#n_estimators is a hyperparam for the number of decision trees used\n",
        "#more trees reduces var but inc time\n",
        "#other hyperparams: max_depth, max_features, min_samples_split, max_samples_split, bootstrap\n",
        "rfRegressor = RandomForestRegressor(random_state = 42)\n",
        "\n",
        "#fit\n",
        "#rfRegressor.fit(xTrain, yTrain)"
      ],
      "metadata": {
        "id": "56WphoBD_t9n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#hyper parameter tuning\n",
        "paramsdistRF = {'max_features': ['sqrt', 'log2', 0.1, 0.25, 0.5, 0.75, 1.0], 'n_estimators': randint(50, 1000), 'max_depth': range(10, 100), 'bootstrap': [True, False]}\n",
        "\n",
        "mseScorer = make_scorer(mean_squared_error, greater_is_better=False)\n",
        "\n",
        "randSearchRF = RandomizedSearchCV(rfRegressor, param_distributions=paramsdistRF, cv=5, verbose=2, random_state=42, n_jobs=-1, n_iter=500, scoring = mseScorer)\n",
        "\n",
        "randSearchRF.fit(xTrainScaled, yTrainScaled)\n",
        "\n",
        "bestParamsRF = randSearchRF.best_params_\n",
        "bestScoreRF = randSearchRF.best_score_\n",
        "\n",
        "# Convert negative MSE back to positive\n",
        "bestMSE = -bestScoreRF\n",
        "\n",
        "print(f\"Best n_estimators: {bestParamsRF['n_estimators']}, Best Max Features: {bestParamsRF['max_features']}, Best MSE: {bestMSE}, Best Depth: {bestParamsRF['max_depth']}, Best Bootstrap: {bestParamsRF['bootstrap']}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZX9f7OmTyANe",
        "outputId": "3db2893a-66ed-4c98-e80f-28f607fccd55"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 500 candidates, totalling 2500 fits\n",
            "Best n_estimators: 397, Best Max Features: 0.1, Best MSE: 1.0399866879022732, Best Depth: 10, Best Bootstrap: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Validation"
      ],
      "metadata": {
        "id": "CLqX37jjuho-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bestRFRegressor = RandomForestRegressor(n_estimators=bestParamsRF['n_estimators'], max_features=bestParamsRF['max_features'], max_depth=bestParamsRF['max_depth'], bootstrap=bestParamsRF['bootstrap'], random_state=42)\n",
        "\n",
        "#fit\n",
        "bestRFRegressor.fit(xTrainScaled, yTrainScaled)\n",
        "\n",
        "#run on validation\n",
        "valPreds = bestRFRegressor.predict(xValScaled)\n",
        "ValPredsOGScale = scalerY.inverse_transform(valPreds)\n",
        "\n",
        "#compute MSE for the multiple outputs\n",
        "scaledFeatureMSEs = mean_squared_error(yValScaled, valPreds, multioutput='raw_values')\n",
        "featureMSEs = mean_squared_error(yVal, ValPredsOGScale, multioutput='raw_values')\n",
        "\n",
        "print(\"MSE for each feature (scaled):\")\n",
        "for i, feature in enumerate(outputLabels):\n",
        "    print(f\"{feature}: {scaledFeatureMSEs[i]}\")\n",
        "\n",
        "print(\"\\n\\nMSE for each feature (scaled):\")\n",
        "for i, feature in enumerate(outputLabels):\n",
        "    print(f\"{feature}: {featureMSEs[i]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gusdd7g8x1Zz",
        "outputId": "dc49af77-e59e-4a1e-f6bd-119fb1f0b010"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MSE for each feature (scaled):\n",
            "valence: 0.9717062233625247\n",
            "liveness: 1.042962981706042\n",
            "time_signature: 0.9323761856325525\n",
            "energy: 1.020407437515904\n",
            "speechiness: 1.1295580008901165\n",
            "instrumentalness: 1.0175008170713864\n",
            "acousticness: 1.0432754171524659\n",
            "tempo: 0.9674812488375852\n",
            "loudness: 1.0662654670406009\n",
            "danceability: 1.0502601963168785\n",
            "key: 1.0508380031841102\n",
            "\n",
            "\n",
            "MSE for each feature (scaled):\n",
            "valence: 0.06543585735308893\n",
            "liveness: 0.06563654733204584\n",
            "time_signature: 0.09729412003352274\n",
            "energy: 0.08470008293580578\n",
            "speechiness: 0.0022885706624759033\n",
            "instrumentalness: 0.16238322117692547\n",
            "acousticness: 0.1341434413801538\n",
            "tempo: 857.8309183124637\n",
            "loudness: 46.69866228038925\n",
            "danceability: 0.027133005058144453\n",
            "key: 15.902355230952384\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Testing"
      ],
      "metadata": {
        "id": "pYLWWbaauj4_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bestRFRegressor = RandomForestRegressor(n_estimators=bestParamsRF['n_estimators'], max_features=bestParamsRF['max_features'], max_depth=bestParamsRF['max_depth'], bootstrap=bestParamsRF['bootstrap'], random_state=42)\n",
        "\n",
        "#fit\n",
        "bestRFRegressor.fit(xTrain, yTrain)\n",
        "\n",
        "#run on validation\n",
        "testPreds = bestRFRegressor.predict(xTest)\n",
        "\n",
        "#compute MSE for the multiple outputs\n",
        "featureMSEs = mean_squared_error(yTest, testPreds, multioutput='raw_values')\n",
        "\n",
        "print(\"MSE for each feature:\")\n",
        "for i, feature in enumerate(outputLabels):\n",
        "    print(f\"{feature}: {featureMSEs[i]}\")"
      ],
      "metadata": {
        "id": "UyYWyTRhujpl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}